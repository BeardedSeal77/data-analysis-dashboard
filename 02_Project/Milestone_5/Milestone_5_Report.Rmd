---
title: "Milestone 5: Deployment and Monitoring/Maintenance"
subtitle: "CRISP-DM Phase 6 - Health and Demographic Patterns in South Africa (HDPSA)"
author: "Group A"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output:
  word_document:
    toc: true
    toc_depth: 2
    number_sections: false
    reference_docx: reference_template.docx
    fig_width: 7
    fig_height: 5
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE)
library(knitr)
library(kableExtra)
```

\newpage

# Executive Summary

This report documents the deployment of the HDPSA Random Forest model as a web-based Survey Data Analytics Tool, completing CRISP-DM Phase 6 (Deployment).

**Key Deliverables:**

- Deployment Strategy (Task 1)
- Functional web deployment at http://localhost:3000/project (Task 2)
- Comprehensive Maintenance Plan (Task 3)
- Automated Monitoring Plan (Task 4)

**Model Performance:** R² = 0.997, RMSE = 0.0554, MAE = 0.0382 (Milestone 4 validation)

**Target Users:** Health researchers, DHS survey planners, data quality teams

**Use Cases:** Survey data validation, gap-filling, consistency analysis

\newpage

# Task 1: Plan Deployment (Business Strategy)

## 1.1 Stakeholder Analysis

### Primary Stakeholders

**Health Ministry Data Quality Teams**

- Need: Validate survey results against historical patterns
- Benefit: Reduced manual review time, improved data integrity

**DHS Survey Planners**

- Need: Estimate expected outcomes for budgeting and sample planning
- Benefit: Better resource allocation and planning efficiency

**Academic Researchers**

- Need: Fill gaps in incomplete historical datasets
- Benefit: More complete datasets for trend analysis

### Business Value

1. Detect data anomalies faster than manual review
2. Support survey planning efficiency
3. Enable longitudinal trend analysis
4. Demonstrate end-to-end ML deployment capability

\newpage

## 1.2 Model Performance Summary

```{r model-metrics}
metrics_table <- data.frame(
  Metric = c("R² Score", "RMSE", "MAE", "Training Samples", "Features"),
  Value = c("0.997", "0.0554", "0.0382", "560", "27"),
  Interpretation = c(
    "Excellent variance explained",
    "Low error on log-scaled target",
    "Mean absolute error",
    "DHS survey records (1998-2016)",
    "Engineered features from Milestone 2"
  )
)

kable(metrics_table,
      caption = "Model Performance Summary",
      align = c("l", "r", "l"))
```

**Key Limitation:** The model achieves excellent fit on held-out data but reflects pattern-matching within the training distribution (1998-2016 aggregated survey statistics) rather than predictive power for individual outcomes or future time periods.

\newpage

## 1.3 Deployment Tool Selection

### Tool Comparison

```{r tool-comparison}
tools_comparison <- data.frame(
  Tool = c("Next.js + Flask + Python", "R Shiny", "Python Streamlit"),
  Scalability = c(9, 4, 6),
  UI_Quality = c(10, 6, 7),
  Infrastructure_Reuse = c(10, 2, 4),
  Total_Score = c(8.8, 5.5, 6.3)
)

kable(tools_comparison,
      caption = "Deployment Tool Comparison (Scores out of 10)",
      align = c("l", rep("c", 4)))
```

### Selected: Next.js + Flask + Python

**Justification:**

1. **Infrastructure Reuse** - Leverages existing Next.js dashboard and Flask API
2. **Scalability** - Microservices architecture allows independent scaling
3. **Professional UI** - React provides polished, responsive interface
4. **Separation of Concerns** - UI, API, and ML logic are independent
5. **Team Familiarity** - Consistent with existing technology stack

**Architecture:**

```
Next.js UI (Port 3000)
    ↓ HTTP REST
Flask API (Port 5001)
    ↓ Python call
ML Service (Python module)
```

\newpage

## 1.4 User Access and Documentation

### Access

- **Development:** http://localhost:3000/project
- **Production (Proposed):** Belgium Campus internal server or cloud hosting (Vercel + Render)

### Documentation

- User Guide with step-by-step instructions
- Model Card with limitations and ethics
- API Documentation with endpoint reference
- FAQ and troubleshooting guide

### Support

- Self-service: FAQ and in-app help
- Email support with 2-day SLA
- Quarterly stakeholder feedback sessions

\newpage

## 1.5 Success Metrics

### Technical Metrics

```{r technical-metrics}
tech_metrics <- data.frame(
  Metric = c("R² Score", "RMSE", "MAE", "API Uptime", "Response Latency"),
  Threshold = c(">= 0.95", "<= 0.07", "<= 0.05", ">= 99.5%", "<= 500ms"),
  Frequency = c("Weekly", "Weekly", "Weekly", "Daily", "Daily")
)

kable(tech_metrics,
      caption = "Technical Performance Monitoring",
      align = c("l", "c", "c"))
```

### Usage Metrics

- Active users per month (target: 10+ by Month 3)
- Prediction requests per week (target: 50+ by Month 2)
- Return user rate (target: 40% retention)

### Business Impact

- Time to validate survey batch (target: 25% reduction)
- Survey planning cycle time (target: 15% reduction)
- Research papers citing the tool

\newpage

# Task 2: Deploy Model (Technical Implementation)

## 2.1 Architecture Overview

The model has been deployed using a three-tier microservices architecture:

**Frontend (Next.js):**

- Location: `app/project/`
- Dashboard: Model status, training interface
- Predictions: CSV upload, results display

**Backend API (Flask):**

- Location: `02_Project/api/app.py`
- Endpoints: `/api/ml/train`, `/api/ml/predict-csv`, `/api/ml/model-info`
- Features: Validation, error handling, CORS

**ML Service (Python):**

- Location: `02_Project/api/ml_service.py`
- Methods: `train_model()`, `predict_from_dataframe()`, `inverse_transform_predictions()`

### Data Flow

1. User uploads CSV via Next.js UI
2. Next.js sends file to Flask API
3. Flask validates and calls ML service
4. ML service preprocesses, predicts, and inverse transforms
5. Flask returns predictions to Next.js
6. Next.js displays results and enables download

\newpage

## 2.2 Key Features

### Model Artifacts

**Saved Files (02_Project/api/ml/outputs/):**

- `final_random_forest_model.pkl` - Trained Random Forest (750 trees)
- `model_metadata.json` - Training date, metrics, sample count
- `feature_importance.json` - Feature rankings
- `categorical_mappings.json` - Encoding dictionaries
- `value_log_scaler.json` - Mean/std for inverse transformation

**Version:** v1.0.0 (initial deployment)

### Input/Output Format

**Input Requirements:**

- CSV with 27 engineered features matching training schema
- Required fields: precision, indicator encoding, sample size tier, data quality scores

**Output Format:**

- `predicted_value_log_scaled` - Scaled prediction for validation
- `predicted_value` - Original percentage/rate (inverse-transformed)

**Example:**

```
Input: BCG vaccine, Age 12-23, High precision, Large sample
Output: 95.2% vaccination rate
```

\newpage

## 2.3 User Interface

### Dashboard Page (`/project`)

- Model Status Card with current metrics (R², RMSE, training date)
- "Train Model" button using local training data
- Navigation to Predictions page
- Quick Start instructions

### Predictions Page (`/project/predict`)

**Features:**

1. File upload (drag-and-drop or click)
2. Preview mode (first 10 predictions)
3. Full CSV download with predictions
4. Results table showing predicted values as percentages
5. Help information and input format guidance

**Results Display:**

- Table columns: Row, Predicted Value (%), Actual Value (%), Difference, Type, Importance
- Color-coded differences (green < 5%, yellow 5-10%, red > 10%)
- Bar chart showing prediction errors

\newpage

## 2.4 Technology Stack

```{r tech-stack}
tech_stack <- data.frame(
  Component = c("Frontend", "Backend API", "ML Framework", "Data Processing"),
  Technology = c("Next.js", "Flask", "scikit-learn", "pandas"),
  Version = c("15.1.0", "3.0.0", "1.3.2", "2.1.0")
)

kable(tech_stack,
      caption = "Technology Stack",
      align = c("l", "l", "c"))
```

### Running Locally

Execute `Z_start-dev.bat` which:

1. Installs npm dependencies
2. Creates Python virtual environments
3. Starts all 3 servers (Next.js, Flask Task API, Flask ML API)

**Access URLs:**

- ML Dashboard: http://localhost:3000/project
- Flask ML API: http://localhost:5001

\newpage

# Task 3: Plan Maintenance

## 3.1 Retraining Procedures

### Retraining Triggers

1. **Performance Degradation:** RMSE > 0.07 or R² < 0.95 for two consecutive weeks
2. **Data Drift:** PSI > 0.2 or KS test p < 0.05 for multiple features
3. **New Data:** Quarterly national survey releases
4. **Business Changes:** Revised KPIs or policy focus

### Retraining Process

1. Collect new/updated data
2. Re-run Milestone 2 (Data Preparation) pipeline
3. Re-run Milestone 3 (Modeling) with hyperparameter tuning
4. Re-run Milestone 4 (Evaluation) on holdout sets
5. Compare to baseline and approve if metrics improve
6. Document and deploy new version

\newpage

## 3.2 Model Versioning

### Semantic Versioning

**Format:** `vMAJOR.MINOR.PATCH`

- **MAJOR:** Breaking schema or architecture changes
- **MINOR:** New data, tuning, or non-breaking feature updates
- **PATCH:** Bug fixes with no material metric change

### Model Registry

**Location:** `02_Project/Milestone_5/model_registry.csv`

**Fields:** version, date, dataset_hash, hyperparameters, rmse, mae, r2, psi, owner, artifact_uri

**Retention Policy:**

- Active: Current production model
- Backup: Previous version (for rollback)
- Archive: Older versions (cold storage)

\newpage

## 3.3 Model Retirement

### Retirement Criteria

1. Persistently low performance for > 3 months
2. Structural data changes undermining learned relationships
3. Documented bias not correctable by retraining
4. Ethical or compliance risks

### Retirement Process

1. Generate final performance report
2. Notify stakeholders
3. Hold formal review meeting
4. Archive model artifacts and logs
5. Disable prediction endpoint
6. Record decision in retirement log

\newpage

## 3.4 Governance Schedule

```{r governance-schedule}
governance_schedule <- data.frame(
  Quarter = c("Q1", "Q2", "Q3", "Q4"),
  Focus = c(
    "Retraining and rollback drill",
    "Backup and disaster recovery test",
    "Documentation and registry audit",
    "Lifecycle and roadmap review"
  ),
  Outcome = c(
    "Validated runbook",
    "Restores verified",
    "Metadata updated",
    "Continue / Retire decision"
  )
)

kable(governance_schedule,
      caption = "Quarterly Governance Schedule",
      align = c("c", "l", "l"))
```

\newpage

# Task 4: Plan Monitoring

## 4.1 Performance Monitoring

### Core Metrics

```{r monitoring-metrics}
monitoring_metrics <- data.frame(
  Metric = c("RMSE", "MAE", "R²"),
  Baseline = c("0.0554", "0.0382", "0.997"),
  Alert_Threshold = c("> 0.07", "> 0.05", "< 0.95"),
  Frequency = c("Weekly", "Weekly", "Weekly")
)

kable(monitoring_metrics,
      caption = "Performance Monitoring Metrics",
      align = c("l", "c", "c", "c"))
```

### Alerting

- **Green:** All metrics within thresholds
- **Amber:** Single metric near threshold
- **Red:** Threshold breach or multiple amber signals
- **Escalation:** Two consecutive amber or one red creates maintenance ticket

\newpage

## 4.2 Data Drift Detection

### Drift Tests

**Input Feature Drift (Kolmogorov-Smirnov Test):**

- Compare current vs training distribution
- Test each top feature
- Flag if p < 0.05

**Population Stability (PSI):**

- Compute PSI per feature
- Flag if PSI > 0.2 (material shift) or > 0.3 (critical shift)

**Output Drift:**

- Track prediction mean/variance over 4-week window
- Flag if shift > 15% relative to baseline

### Remediation

```{r drift-remediation}
drift_remediation <- data.frame(
  Severity = c("Minor", "Moderate", "Severe"),
  Criteria = c("PSI < 0.2", "PSI 0.2-0.3", "PSI > 0.3"),
  Action = c(
    "Log and monitor next cycle",
    "Schedule data refresh and review",
    "Trigger retraining"
  )
)

kable(drift_remediation,
      caption = "Drift Remediation Matrix",
      align = c("l", "l", "l"))
```

\newpage

## 4.3 Automation Workflow (n8n)

### Workflow Overview

Weekly automated monitoring using n8n:

1. **Scheduler** - Triggers Mondays 07:00 SAST
2. **Fetch Data** - Calls Flask `/api/metrics/window`
3. **Compute Metrics** - Calculates RMSE, MAE, R²
4. **KS & PSI Tests** - Runs drift detection
5. **Render Report** - Generates weekly report
6. **Alert** - Sends Slack/email if thresholds breached
7. **Dashboard Refresh** - Updates monitoring pages

**Storage:** MongoDB for logs, metrics, and drift results

**Security:** API keys stored in n8n credentials (encrypted)

\newpage

## 4.4 Ethical Monitoring

### Fairness Tests

**Subgroup Error Parity:**

- Compute MAE/RMSE across provinces, urban/rural, indicator domains
- Flag if subgroup MAE deviates > 25% from overall MAE

**Calibration Parity:**

- Quarterly review of calibration plots
- Ensure predictions not systematically biased

### Privacy (POPIA Compliance)

- No personal identifiers processed
- Aggregated data only
- Logs de-identified
- 12-month retention, then deletion

\newpage

# Ethical Considerations

## Scope and Limitations

### In Scope

- Survey indicator value prediction based on metadata
- Data quality validation
- Historical gap-filling
- Performance monitoring

### Out of Scope

- Individual health outcome prediction (not household-level data)
- Causal analysis (cannot link domains like water → health)
- Time series forecasting beyond 2016
- Real-time streaming (batch processing only)

### Critical Limitations

- Aggregated statistics, not individual records
- Cannot establish causal relationships
- Limited to 27 specific features
- Historical patterns (1998-2016) may not generalize

## Fairness and Bias

**Mitigation Strategies:**

- Subgroup error analysis across provinces
- Document limitations in Model Card
- Alert users when input differs from training distribution
- Clear documentation that predictions are population-level only

## Transparency

**Model Card includes:**

- Purpose, training data sources, performance metrics
- Known limitations and failure modes
- Ethical considerations and recommended use cases
- Contact information for concerns

\newpage

# Risk Assessment

```{r risk-assessment}
risk_assessment <- data.frame(
  Risk = c(
    "Model predictions degrade over time",
    "Low user adoption",
    "Misuse for unsupported cases",
    "Security vulnerabilities"
  ),
  Likelihood = c("Medium", "Medium", "Medium", "Medium"),
  Impact = c("High", "Medium", "Critical", "High"),
  Mitigation = c(
    "Automated monitoring, quarterly retraining reviews",
    "Clear documentation, stakeholder demos, training",
    "Prominent disclaimers, user education, access restrictions",
    "Input sanitization, file limits, security audit"
  )
)

kable(risk_assessment,
      caption = "Risk Assessment Summary",
      align = c("l", "c", "c", "l"))
```

\newpage

# Conclusion

## Key Accomplishments

This Milestone 5 report documents successful deployment of the HDPSA Random Forest model, completing CRISP-DM Phase 6:

1. **Strategic Planning** - Stakeholder analysis, tool selection, success metrics
2. **Technical Implementation** - 3-tier microservices architecture with professional UI
3. **Maintenance Strategy** - Retraining procedures, versioning, retirement policy
4. **Monitoring Plan** - Automated performance tracking and drift detection

## Model Positioning

**Legitimate Use Cases:**

- Survey data quality validation
- Gap-filling for incomplete datasets
- Survey planning and estimation
- Consistency analysis (1998-2016 distribution)

**Acknowledged Limitations:**

- Population-level patterns only
- No causal inference
- Historical context bound (1998-2016)
- Pattern-matching within training distribution

## Recommendations

### Short-Term (3 Months)

1. Complete MongoDB logging integration
2. Implement n8n monitoring workflow
3. Conduct stakeholder training
4. Security hardening and testing

### Medium-Term (6-12 Months)

1. Model retraining with additional data (2017-2024)
2. Add confidence intervals on predictions
3. Docker containerization
4. Video demos and case studies

### Long-Term (12+ Months)

1. Explore domain-specific models
2. Integration with DHS data platforms
3. Research collaboration and publications

\newpage

# Appendices

## Appendix A: Model Card Summary

- **Model:** HDPSA Random Forest v1.0.0
- **Performance:** R² = 0.997, RMSE = 0.0554
- **Training Data:** 560 DHS survey records (1998-2016)
- **Use Cases:** Validation, planning, gap-filling
- **Limitations:** Population-level only, no causal inference

## Appendix B: API Endpoints

```{r api-endpoints}
api_endpoints <- data.frame(
  Method = c("GET", "GET", "POST", "POST", "POST"),
  Endpoint = c(
    "/api/health",
    "/api/ml/model-info",
    "/api/ml/train",
    "/api/ml/predict-csv",
    "/api/ml/predict-sample"
  ),
  Description = c(
    "Service health check",
    "Model metadata and metrics",
    "Train model using local data",
    "Full CSV predictions",
    "Preview (10 rows) predictions"
  )
)

kable(api_endpoints,
      caption = "Flask API Endpoints",
      align = c("c", "l", "l"))
```

## Appendix C: File Structure

```
02_Project/
├── api/
│   ├── app.py                    (Flask API)
│   ├── ml_service.py             (ML service)
│   └── ml/outputs/               (Model artifacts)
├── Milestone_5/
│   ├── Task_1_Deployment_Strategy.md
│   ├── Task_3_Maintenance_Plan.md
│   ├── Task_4_Monitoring_Plan.md
│   └── Milestone_5_Report.Rmd
app/
└── project/
    ├── page.tsx                  (Dashboard)
    └── predict/page.tsx          (Predictions)
```

---

**Document Version:** 1.0
**Date:** `r format(Sys.Date(), '%d %B %Y')`
**Status:** Submitted for Assessment
