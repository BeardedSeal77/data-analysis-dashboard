---
title: "Task 3: Random Forest Implementation"
author: "ED Cullen"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  message = FALSE
)
```

# Overview

Task 3: Implement Random Forest regression with systematic hyperparameter tuning.

---

# 1. Setup and Data Loading

```{r load_libraries}
library(randomForest)
library(tidyverse)
```

```{r global_variables}
# Auto-detect environment and set paths
current_dir <- basename(getwd())
if (current_dir == "Task_03") {
  train_path <- "../../Data/04_Split/train_data.csv"
  test_path <- "../../Data/04_Split/test_data.csv"
  outputs_path <- "outputs"
} else {
  train_path <- "02_Project/Data/04_Split/train_data.csv"
  test_path <- "02_Project/Data/04_Split/test_data.csv"
  outputs_path <- "02_Project/Milestone_3/Task_03/outputs"
}

if (!dir.exists(outputs_path)) {
  dir.create(outputs_path, recursive = TRUE)
}
```

```{r load_data}
train_data <- read.csv(train_path)
test_data <- read.csv(test_path)
```

---

# 2. Baseline Model

```{r baseline_model}
# Train baseline model with default parameters
baseline_rf <- randomForest(
  value_log_scaled ~ .,
  data = train_data,
  ntree = 500,
  importance = TRUE
)

print(baseline_rf)
```

---

# 3. Hyperparameter Tuning

## 3.1 Tuning ntree

```{r tune_ntree}
ntree_values <- c(500, 750, 1000, 1250, 1500, 2000)
ntree_results <- data.frame(ntree = integer(), OOB_Error = numeric(), Test_RMSE = numeric())

for (n_trees in ntree_values) {
  rf_model <- randomForest(
    value_log_scaled ~ .,
    data = train_data,
    ntree = n_trees,
    mtry = floor(sqrt(ncol(train_data) - 1))
  )

  oob_error <- tail(rf_model$mse, 1)
  pred <- predict(rf_model, newdata = test_data)
  test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))

  ntree_results <- rbind(ntree_results, data.frame(
    ntree = n_trees,
    OOB_Error = oob_error,
    Test_RMSE = test_rmse
  ))
}

print(ntree_results)
optimal_ntree <- ntree_results$ntree[which.min(ntree_results$Test_RMSE)]
cat("Optimal ntree:", optimal_ntree, "\n")
```

## 3.2 Tuning mtry

```{r tune_mtry}
num_features <- ncol(train_data) - 1
mtry_values <- unique(floor(seq(sqrt(num_features), num_features/3, length.out = 6)))
mtry_results <- data.frame(mtry = integer(), OOB_Error = numeric(), Test_RMSE = numeric())

for (m in mtry_values) {
  rf_model <- randomForest(
    value_log_scaled ~ .,
    data = train_data,
    ntree = optimal_ntree,
    mtry = m
  )

  oob_error <- tail(rf_model$mse, 1)
  pred <- predict(rf_model, newdata = test_data)
  test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))

  mtry_results <- rbind(mtry_results, data.frame(
    mtry = m,
    OOB_Error = oob_error,
    Test_RMSE = test_rmse
  ))
}

print(mtry_results)
optimal_mtry <- mtry_results$mtry[which.min(mtry_results$Test_RMSE)]
cat("Optimal mtry:", optimal_mtry, "\n")
```

## 3.3 Tuning nodesize

```{r tune_nodesize}
nodesize_values <- c(1, 2, 3, 5, 7, 10)
nodesize_results <- data.frame(nodesize = integer(), OOB_Error = numeric(), Test_RMSE = numeric())

for (ns in nodesize_values) {
  rf_model <- randomForest(
    value_log_scaled ~ .,
    data = train_data,
    ntree = optimal_ntree,
    mtry = optimal_mtry,
    nodesize = ns
  )

  oob_error <- tail(rf_model$mse, 1)
  pred <- predict(rf_model, newdata = test_data)
  test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))

  nodesize_results <- rbind(nodesize_results, data.frame(
    nodesize = ns,
    OOB_Error = oob_error,
    Test_RMSE = test_rmse
  ))
}

print(nodesize_results)
optimal_nodesize <- nodesize_results$nodesize[which.min(nodesize_results$Test_RMSE)]
cat("Optimal nodesize:", optimal_nodesize, "\n")
```

---

# 4. Final Model

```{r final_model}
# Train final model with optimized parameters
final_rf <- randomForest(
  value_log_scaled ~ .,
  data = train_data,
  ntree = optimal_ntree,
  mtry = optimal_mtry,
  nodesize = optimal_nodesize,
  importance = TRUE,
  keep.forest = TRUE
)

print(final_rf)

# Save model
saveRDS(final_rf, file.path(outputs_path, "final_random_forest_model.rds"))

# Save tuning results
write.csv(ntree_results, file.path(outputs_path, "ntree_tuning.csv"), row.names = FALSE)
write.csv(mtry_results, file.path(outputs_path, "mtry_tuning.csv"), row.names = FALSE)
write.csv(nodesize_results, file.path(outputs_path, "nodesize_tuning.csv"), row.names = FALSE)
```

---

# 5. Parameter Documentation

## ntree (Number of Trees)
- **Tested:** 500-2000
- **Selected:** `r optimal_ntree`
- **Rationale:** Minimizes test RMSE while ensuring OOB error convergence

## mtry (Variables per Split)
- **Tested:** `r min(mtry_values)` to `r max(mtry_values)`
- **Selected:** `r optimal_mtry`
- **Rationale:** Balances tree diversity with prediction accuracy

## nodesize (Minimum Node Size)
- **Tested:** 1-10
- **Selected:** `r optimal_nodesize`
- **Rationale:** Prevents overfitting while maintaining model complexity

---

# Session Information

```{r session_info}
sessionInfo()
```
