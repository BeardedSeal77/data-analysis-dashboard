knitr::opts_chunk$set(
echo = TRUE,
warning = FALSE,
message = FALSE
)
library(randomForest)
library(tidyverse)
# Auto-detect environment and set paths
current_dir <- basename(getwd())
if (current_dir == "Task_03") {
train_path <- "../../Data/04_Split/train_data.csv"
test_path <- "../../Data/04_Split/test_data.csv"
outputs_path <- "outputs"
} else {
train_path <- "02_Project/Data/04_Split/train_data.csv"
test_path <- "02_Project/Data/04_Split/test_data.csv"
outputs_path <- "02_Project/Milestone_3/Task_03/outputs"
}
if (!dir.exists(outputs_path)) {
dir.create(outputs_path, recursive = TRUE)
}
train_data <- read.csv(train_path)
test_data <- read.csv(test_path)
# Train baseline model with default parameters
baseline_rf <- randomForest(
value_log_scaled ~ .,
data = train_data,
ntree = 500,
importance = TRUE
)
print(baseline_rf)
ntree_values <- c(500, 750, 1000, 1250, 1500, 2000)
ntree_results <- data.frame(ntree = integer(), OOB_Error = numeric(), Test_RMSE = numeric())
for (n_trees in ntree_values) {
rf_model <- randomForest(
value_log_scaled ~ .,
data = train_data,
ntree = n_trees,
mtry = floor(sqrt(ncol(train_data) - 1))
)
oob_error <- tail(rf_model$mse, 1)
pred <- predict(rf_model, newdata = test_data)
test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))
ntree_results <- rbind(ntree_results, data.frame(
ntree = n_trees,
OOB_Error = oob_error,
Test_RMSE = test_rmse
))
}
print(ntree_results)
optimal_ntree <- ntree_results$ntree[which.min(ntree_results$Test_RMSE)]
cat("Optimal ntree:", optimal_ntree, "\n")
num_features <- ncol(train_data) - 1
mtry_values <- unique(floor(seq(sqrt(num_features), num_features/3, length.out = 6)))
mtry_results <- data.frame(mtry = integer(), OOB_Error = numeric(), Test_RMSE = numeric())
for (m in mtry_values) {
rf_model <- randomForest(
value_log_scaled ~ .,
data = train_data,
ntree = optimal_ntree,
mtry = m
)
oob_error <- tail(rf_model$mse, 1)
pred <- predict(rf_model, newdata = test_data)
test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))
mtry_results <- rbind(mtry_results, data.frame(
mtry = m,
OOB_Error = oob_error,
Test_RMSE = test_rmse
))
}
print(mtry_results)
optimal_mtry <- mtry_results$mtry[which.min(mtry_results$Test_RMSE)]
cat("Optimal mtry:", optimal_mtry, "\n")
nodesize_values <- c(1, 2, 3, 5, 7, 10)
nodesize_results <- data.frame(nodesize = integer(), OOB_Error = numeric(), Test_RMSE = numeric())
for (ns in nodesize_values) {
rf_model <- randomForest(
value_log_scaled ~ .,
data = train_data,
ntree = optimal_ntree,
mtry = optimal_mtry,
nodesize = ns
)
oob_error <- tail(rf_model$mse, 1)
pred <- predict(rf_model, newdata = test_data)
test_rmse <- sqrt(mean((test_data$value_log_scaled - pred)^2))
nodesize_results <- rbind(nodesize_results, data.frame(
nodesize = ns,
OOB_Error = oob_error,
Test_RMSE = test_rmse
))
}
print(nodesize_results)
optimal_nodesize <- nodesize_results$nodesize[which.min(nodesize_results$Test_RMSE)]
cat("Optimal nodesize:", optimal_nodesize, "\n")
# Train final model with optimized parameters
final_rf <- randomForest(
value_log_scaled ~ .,
data = train_data,
ntree = optimal_ntree,
mtry = optimal_mtry,
nodesize = optimal_nodesize,
importance = TRUE,
keep.forest = TRUE
)
print(final_rf)
# Save model
saveRDS(final_rf, file.path(outputs_path, "final_random_forest_model.rds"))
# Save tuning results
write.csv(ntree_results, file.path(outputs_path, "ntree_tuning.csv"), row.names = FALSE)
write.csv(mtry_results, file.path(outputs_path, "mtry_tuning.csv"), row.names = FALSE)
write.csv(nodesize_results, file.path(outputs_path, "nodesize_tuning.csv"), row.names = FALSE)
sessionInfo()
