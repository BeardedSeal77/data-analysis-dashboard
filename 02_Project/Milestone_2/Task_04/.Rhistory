knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
fig.width = 10, fig.height = 8)
# Load required libraries
suppressMessages({
library(tidyverse)
library(corrplot)
library(pheatmap)
library(DT)
library(kableExtra)
})
cat("Analysis environment ready\n")
# Set data path
data_path <- "../../Data/01_Raw/"
# List all CSV files
csv_files <- list.files(data_path, pattern = "*.csv", full.names = TRUE)
cat("Found", length(csv_files), "CSV files\n")
# Load all datasets
datasets <- list()
for (file in csv_files) {
dataset_name <- gsub(".csv", "", basename(file))
datasets[[dataset_name]] <- read_csv(file, show_col_types = FALSE)
}
cat("Loaded", length(datasets), "datasets successfully\n")
# Combine all datasets
combined_data <- bind_rows(datasets, .id = "dataset_source")
# Clean and prepare data for analysis
analysis_data <- combined_data %>%
filter(!is.na(Value)) %>%
select(dataset_source, Indicator, Value, SurveyYear) %>%
mutate(
Value = as.numeric(Value),
SurveyYear = as.numeric(SurveyYear)
) %>%
filter(!is.na(Value), !is.infinite(Value))
cat("Combined dataset:", nrow(analysis_data), "observations\n")
# Check indicator coverage
indicator_coverage <- analysis_data %>%
group_by(Indicator) %>%
summarise(
n_datasets = n_distinct(dataset_source),
n_observations = n(),
.groups = 'drop'
) %>%
arrange(desc(n_datasets))
cat("Total unique indicators:", nrow(indicator_coverage), "\n")
# Save indicator coverage for review
write.csv(indicator_coverage, "indicator_coverage_summary.csv", row.names = FALSE)
cat("Indicator coverage saved to indicator_coverage_summary.csv\n")
# Use dataset_source as grouping for better correlation analysis
correlation_matrix_data <- analysis_data %>%
group_by(Indicator, dataset_source) %>%
summarise(avg_value = mean(Value, na.rm = TRUE), .groups = 'drop') %>%
pivot_wider(names_from = Indicator, values_from = avg_value) %>%
select(-dataset_source)
# Handle missing values and constant columns
correlation_matrix_data <- correlation_matrix_data %>%
mutate_all(~ ifelse(is.na(.), mean(., na.rm = TRUE), .)) %>%
select_if(~ var(., na.rm = TRUE) > 0)
cat("Correlation matrix dimensions:", nrow(correlation_matrix_data), "x", ncol(correlation_matrix_data), "\n")
# Preview the correlation matrix data structure
cat("Sample of indicators included in correlation analysis:\n")
print(names(correlation_matrix_data)[1:min(5, ncol(correlation_matrix_data))])
# Calculate correlation matrix
numeric_data <- correlation_matrix_data %>% select_if(is.numeric)
cor_matrix <- cor(numeric_data, use = "complete.obs")
cat("Final correlation matrix:", nrow(cor_matrix), "x", ncol(cor_matrix), "\n")
cat("Range of correlations:", round(min(cor_matrix, na.rm = TRUE), 3),
"to", round(max(cor_matrix, na.rm = TRUE), 3), "\n")
# Save correlation matrix for review
write.csv(cor_matrix, "correlation_matrix.csv")
cat("Correlation matrix saved to correlation_matrix.csv\n")
# Create correlation heatmap
if (ncol(cor_matrix) > 1) {
pheatmap(cor_matrix,
main = "Correlation Heatmap of Health Indicators",
color = colorRampPalette(c("red", "white", "blue"))(100),
breaks = seq(-1, 1, length.out = 101),
display_numbers = TRUE,
fontsize = 10,
angle_col = 45,
number_format = "%.2f")
cat("Heatmap generated successfully\n")
} else {
cat("Insufficient data for correlation heatmap\n")
}
# Find significant correlations
cor_threshold <- 0.7
significant_cors <- which(abs(cor_matrix) > cor_threshold & cor_matrix != 1, arr.ind = TRUE)
if (nrow(significant_cors) > 0) {
significant_pairs <- data.frame(
Indicator1 = rownames(cor_matrix)[significant_cors[,1]],
Indicator2 = colnames(cor_matrix)[significant_cors[,2]],
Correlation = cor_matrix[significant_cors],
Strength = ifelse(cor_matrix[significant_cors] > 0, "Positive", "Negative"),
Abs_Correlation = abs(cor_matrix[significant_cors])
) %>%
arrange(desc(Abs_Correlation))
cat("SIGNIFICANT CORRELATIONS FOUND:\n")
cat("Total significant correlations (|r| >", cor_threshold, "):", nrow(significant_pairs), "\n")
cat("Strongest positive correlation:", round(max(significant_pairs$Correlation), 3), "\n")
if (any(significant_pairs$Correlation < 0)) {
cat("Strongest negative correlation:", round(min(significant_pairs$Correlation), 3), "\n")
}
# Display as formatted table
kable(significant_pairs,
caption = paste("Significant Correlations (|r| >", cor_threshold, ")"),
digits = 3) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save significant correlations
write.csv(significant_pairs, "significant_correlations.csv", row.names = FALSE)
cat("Significant correlations saved to significant_correlations.csv\n")
} else {
cat("NO SIGNIFICANT CORRELATIONS FOUND\n")
cat("No correlations above threshold of", cor_threshold, "\n")
cat("Features are relatively independent (beneficial for modeling)\n")
}
# Calculate feature variance as importance measure
feature_variance <- numeric_data %>%
summarise_all(~ var(., na.rm = TRUE)) %>%
pivot_longer(everything(), names_to = "Feature", values_to = "Variance") %>%
arrange(desc(Variance)) %>%
mutate(
Rank = row_number(),
Normalized_Variance = scales::rescale(Variance, to = c(0, 1)),
Combined_Score = Normalized_Variance,
Final_Rank = Rank
)
cat("Total features analyzed:", nrow(feature_variance), "\n")
cat("Ranking method: Variance-based importance\n")
# Display top features
kable(feature_variance,
caption = "Feature Importance Rankings by Variance",
digits = 4) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save feature importance results
write.csv(feature_variance, "feature_importance_rankings.csv", row.names = FALSE)
cat("Feature importance rankings saved to feature_importance_rankings.csv\n")
# Create feature importance plot
importance_plot <- ggplot(feature_variance,
aes(x = reorder(Feature, Combined_Score), y = Combined_Score)) +
geom_col(fill = "steelblue", alpha = 0.7) +
coord_flip() +
labs(
title = "Health Indicator Importance Rankings",
subtitle = "Ranked by variance (higher variance = greater discriminatory power)",
x = "Health Indicators",
y = "Normalized Importance Score"
) +
theme_minimal() +
theme(
axis.text.y = element_text(size = 9),
plot.title = element_text(size = 14, face = "bold"),
plot.subtitle = element_text(size = 11, color = "gray50")
)
print(importance_plot)
cat("Feature importance plot generated\n")
# Create weighting strategy based on importance rankings
weight_strategy <- feature_variance %>%
mutate(
Weight_Category = case_when(
Final_Rank <= 5 ~ "High Weight (3x)",
Final_Rank <= 15 ~ "Medium Weight (2x)",
Final_Rank <= 30 ~ "Standard Weight (1x)",
TRUE ~ "Low Weight (0.5x)"
),
Suggested_Weight = case_when(
Final_Rank <= 5 ~ 3.0,
Final_Rank <= 15 ~ 2.0,
Final_Rank <= 30 ~ 1.0,
TRUE ~ 0.5
)
) %>%
select(Feature, Final_Rank, Combined_Score, Weight_Category, Suggested_Weight)
# Weight distribution summary
weight_summary <- weight_strategy %>%
count(Weight_Category, name = "Number_of_Features") %>%
arrange(desc(Number_of_Features))
cat("WEIGHT DISTRIBUTION SUMMARY:\n")
kable(weight_summary,
caption = "Distribution of Features Across Weight Categories") %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Detailed weight recommendations
cat("\nDETAILED WEIGHT RECOMMENDATIONS:\n")
kable(weight_strategy,
caption = "Suggested Feature Weights for Modeling Phase",
digits = 4) %>%
kable_styling(bootstrap_options = c("striped", "hover"))
# Save weight strategy
write.csv(weight_strategy, "weight_attribution_strategy.csv", row.names = FALSE)
cat("Weight strategy saved to weight_attribution_strategy.csv\n")
cat("=== TASK 4 DELIVERABLES COMPLETED ===\n\n")
cat("CORRELATION ANALYSIS RESULTS:\n")
cat("- Correlation matrix dimensions:", nrow(cor_matrix), "x", ncol(cor_matrix), "\n")
cat("- Correlation range:", round(min(cor_matrix, na.rm = TRUE), 3),
"to", round(max(cor_matrix, na.rm = TRUE), 3), "\n")
if (exists("significant_pairs") && nrow(significant_pairs) > 0) {
cat("- Significant correlations found:", nrow(significant_pairs), "\n")
cat("- Multicollinearity considerations required for modeling\n")
} else {
cat("- No significant correlations above threshold\n")
cat("- Features are relatively independent (ideal for modeling)\n")
}
cat("\nFEATURE IMPORTANCE RANKINGS:\n")
cat("- Total features analyzed:", nrow(feature_variance), "\n")
cat("- Top feature:", feature_variance$Feature[1], "\n")
cat("- Ranking method: Variance-based discriminatory power\n")
cat("\nWEIGHT ATTRIBUTION STRATEGY:\n")
for (i in 1:nrow(weight_summary)) {
cat("- ", weight_summary$Weight_Category[i], ":", weight_summary$Number_of_Features[i], "features\n")
}
cat("\nR/R MARKDOWN CODE:\n")
cat("- Statistical analysis implementation complete\n")
cat("- All code documented and reproducible\n")
cat("- Results exported to CSV files for integration\n")
cat("\n=== INTEGRATION WITH TASK 5 ===\n")
cat("Key findings ready for data selection criteria refinement:\n")
cat("1. Correlation insights for multicollinearity management\n")
cat("2. Evidence-based feature importance hierarchy\n")
cat("3. Quantitative weight recommendations for modeling\n")
# Save key R objects for Task 5 integration
save(feature_variance, cor_matrix, weight_strategy, significant_pairs,
file = "task_04_results.RData")
cat("All results saved to task_04_results.RData for Task 5 integration\n")
cat("CSV files created for external review and documentation\n")
