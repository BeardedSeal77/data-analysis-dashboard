missing_summary <- final_features %>%
summarise(across(everything(), ~ sum(is.na(.x)))) %>%
pivot_longer(everything(), names_to = "feature", values_to = "missing_count") %>%
filter(missing_count > 0)
suppressPackageStartupMessages({
library(readr)     # read_csv, write_csv
library(dplyr)     # select, mutate, across, etc.
library(tidyr)     # pivot_longer, pivot_wider
library(rsample)   # initial_split, training, testing
library(recipes)   # pre-processing pipeline
library(knitr)     # pretty tables in report
})
# Auto-detect environment and set paths
current_dir <- basename(getwd())
if (current_dir == "Task_04") {
# Running in RStudio (current directory is Task_04)
cleaned_data_path <- "../../Data/02_Cleaned"
outputs_path <- "../../Data/03_Scaled"
cat("Environment: RStudio\n")
} else {
# Running in VS Code (from project root)
cleaned_data_path <- "02_Project/Data/02_Cleaned"
outputs_path <- "02_Project/Data/03_Scaled"
cat("Environment: VS Code\n")
}
# Create outputs directory
if (!dir.exists(outputs_path)) {
dir.create(outputs_path, recursive = TRUE)
cat("Created outputs directory:", outputs_path, "\n")
}
# Feature engineering thresholds
RARE_CATEGORY_THRESHOLD <- 0.05   # Group categories with <5% frequency
HIGH_CARDINALITY_THRESHOLD <- 50  # Max categories before grouping
SAMPLE_SIZE_SMALL <- 1000         # Small sample threshold
SAMPLE_SIZE_LARGE <- 10000        # Large sample threshold
cat("Global variables set successfully\n")
cat("Cleaned data path:", cleaned_data_path, "\n")
cat("Outputs path:", outputs_path, "\n")
# List all cleaned CSV files (only the 7 selected datasets)
selected_datasets <- c(
"access-to-health-care_national_zaf",
"immunization_national_zaf",
"hiv-behavior_national_zaf",
"water_national_zaf",
"dhs-quickstats_national_zaf",
"toilet-facilities_national_zaf",
"child-mortality-rates_national_zaf"
)
# Load cleaned datasets (check for _final.csv suffix)
cleaned_csv_files <- paste0(file.path(cleaned_data_path, selected_datasets), "_final.csv")
existing_files <- cleaned_csv_files[file.exists(cleaned_csv_files)]
# If _final.csv doesn't exist, try without suffix
if (length(existing_files) == 0) {
cleaned_csv_files <- paste0(file.path(cleaned_data_path, selected_datasets), ".csv")
existing_files <- cleaned_csv_files[file.exists(cleaned_csv_files)]
}
cat("Found", length(existing_files), "cleaned datasets:\n")
for(file in existing_files) {
cat("- ", basename(file), "\n")
}
# Load all cleaned datasets into a list and add dataset_source column
if (length(existing_files) > 0) {
datasets <- lapply(existing_files, function(file_path) {
df <- read_csv(file_path, show_col_types = FALSE)
# Add dataset_source column (use file-name without extension)
df$dataset_source <- tools::file_path_sans_ext(basename(file_path))
return(df)
})
# Combine all datasets row-wise
combined_df <- bind_rows(datasets)
# Preview combined dataset
cat("Combined all datasets with dataset_source. Dimensions:", dim(combined_df), "\n")
cat("Total records:", nrow(combined_df), "\n")
cat("Total fields:", ncol(combined_df), "\n\n")
# Show dataset distribution
cat("Records per dataset:\n")
table(combined_df$dataset_source)
glimpse(combined_df)
} else {
cat("ERROR: No cleaned datasets found. Run Task 3 first.\n")
}
# Define field categories based on Task 2 analysis
categorical_fields <- c(
"by_variable_id",
"indicator",
"indicator_type",
"characteristic_category",
"denominator_unweighted",  # Treat as categorical due to repetitive pattern
"dataset_source"           # Added during combination
)
numeric_fields <- c(
"value",                   # Target variable
"precision",
"characteristic_order",
"indicator_order"
)
binary_fields <- c(
"is_preferred"
)
identifier_fields <- c(
"data_id"
)
# Show field type assignment
cat("=== FIELD TYPE ASSIGNMENT ===\n")
cat("Categorical fields (", length(categorical_fields), "):\n")
cat(paste("-", categorical_fields, collapse = "\n"), "\n\n")
cat("Numeric fields (", length(numeric_fields), "):\n")
cat(paste("-", numeric_fields, collapse = "\n"), "\n\n")
cat("Binary fields (", length(binary_fields), "):\n")
cat(paste("-", binary_fields, collapse = "\n"), "\n\n")
cat("Identifier fields (", length(identifier_fields), "):\n")
cat(paste("-", identifier_fields, collapse = "\n"), "\n\n")
# Data quality overview
if (exists("combined_df")) {
cat("=== DATA OVERVIEW ===\n")
cat("Total records:", nrow(combined_df), "\n")
cat("Total fields:", ncol(combined_df), "\n")
cat("Missing values per field:\n")
print(sapply(combined_df, function(x) sum(is.na(x))))
} else {
cat("=== DATA OVERVIEW ===\n")
cat("Error: combined_df not found. Make sure datasets were loaded successfully.\n")
}
# Create working dataframe for feature engineering
df_features <- combined_df
# 1. INDICATOR - Label encode (high cardinality ~50+ categories)
cat("=== ENCODING INDICATOR ===\n")
indicator_counts <- table(df_features$indicator)
cat("Unique indicators:", length(indicator_counts), "\n")
cat("Top 5 most frequent:\n")
print(head(sort(indicator_counts, decreasing = TRUE), 5))
# Label encode indicator
df_features$indicator_encoded <- as.numeric(as.factor(df_features$indicator))
# 2. BY_VARIABLE_ID - One-hot encode with rare category grouping
cat("\n=== ENCODING BY_VARIABLE_ID ===\n")
by_var_counts <- table(df_features$by_variable_id)
cat("Unique by_variable_id:", length(by_var_counts), "\n")
# Group rare categories (< 5% frequency)
total_records <- nrow(df_features)
rare_threshold <- RARE_CATEGORY_THRESHOLD * total_records
rare_categories <- names(by_var_counts[by_var_counts < rare_threshold])
df_features$by_variable_id_grouped <- ifelse(
df_features$by_variable_id %in% rare_categories,
"Other",
df_features$by_variable_id
)
cat("Grouped", length(rare_categories), "rare categories into 'Other'\n")
cat("Final categories:", length(unique(df_features$by_variable_id_grouped)), "\n")
# One-hot encode by_variable_id_grouped
by_var_dummies <- model.matrix(~ by_variable_id_grouped - 1, data = df_features)
colnames(by_var_dummies) <- paste0("by_var_", gsub("by_variable_id_grouped", "", colnames(by_var_dummies)))
# 3. INDICATOR_TYPE - One-hot encode
cat("\n=== ENCODING INDICATOR_TYPE ===\n")
type_counts <- table(df_features$indicator_type)
cat("Unique indicator types:", length(type_counts), "\n")
print(type_counts)
# One-hot encode indicator_type
type_dummies <- model.matrix(~ indicator_type - 1, data = df_features)
colnames(type_dummies) <- paste0("type_", gsub("indicator_type", "", colnames(type_dummies)))
# 4. CHARACTERISTIC_CATEGORY - One-hot encode
cat("\n=== ENCODING CHARACTERISTIC_CATEGORY ===\n")
char_counts <- table(df_features$characteristic_category)
cat("Unique characteristic categories:", length(char_counts), "\n")
print(char_counts)
# One-hot encode characteristic_category
char_dummies <- model.matrix(~ characteristic_category - 1, data = df_features)
colnames(char_dummies) <- paste0("char_", gsub("characteristic_category", "", colnames(char_dummies)))
# 5. DENOMINATOR_UNWEIGHTED - Treat as categorical (survey cohorts)
cat("\n=== ENCODING DENOMINATOR_UNWEIGHTED ===\n")
denom_counts <- table(df_features$denominator_unweighted)
cat("Unique denominator values (survey cohorts):", length(denom_counts), "\n")
print(denom_counts)
# Label encode denominator_unweighted as survey cohorts
df_features$survey_cohort <- as.numeric(as.factor(df_features$denominator_unweighted))
# 6. DATASET_SOURCE - Label encode
cat("\n=== ENCODING DATASET_SOURCE ===\n")
dataset_counts <- table(df_features$dataset_source)
cat("Datasets:", length(dataset_counts), "\n")
print(dataset_counts)
# Label encode dataset_source
df_features$dataset_source_encoded <- as.numeric(as.factor(df_features$dataset_source))
cat("\n=== CATEGORICAL ENCODING COMPLETE ===\n")
# 1. PRECISION - Create precision quality categories and binary feature
cat("=== PRECISION FEATURE ENGINEERING ===\n")
precision_summary <- summary(df_features$precision)
print(precision_summary)
# Create precision quality bins: High (0-1), Medium (2), Low (3+)
df_features$precision_quality <- case_when(
df_features$precision <= 1 ~ "High",
df_features$precision == 2 ~ "Medium",
df_features$precision >= 3 ~ "Low",
TRUE ~ "Unknown"
)
# Create binary high precision feature
df_features$high_precision <- as.numeric(df_features$precision <= 1)
cat("Precision quality distribution:\n")
print(table(df_features$precision_quality))
# 2. CHARACTERISTIC_ORDER - Keep ordinal, check if quintiles needed
cat("\n=== CHARACTERISTIC_ORDER FEATURE ENGINEERING ===\n")
char_order_summary <- summary(df_features$characteristic_order)
print(char_order_summary)
# Create quintiles if wide range
range_char_order <- max(df_features$characteristic_order, na.rm = TRUE) - min(df_features$characteristic_order, na.rm = TRUE)
if (range_char_order > 10) {
df_features$char_order_quintile <- ntile(df_features$characteristic_order, 5)
cat("Created characteristic order quintiles (range =", range_char_order, ")\n")
} else {
df_features$char_order_quintile <- df_features$characteristic_order
cat("Kept original characteristic order (range =", range_char_order, ")\n")
}
# 3. INDICATOR_ORDER - Create importance tiers
cat("\n=== INDICATOR_ORDER FEATURE ENGINEERING ===\n")
indicator_order_summary <- summary(df_features$indicator_order)
print(indicator_order_summary)
# Create indicator importance tiers: High (1-3), Medium (4-6), Low (7+)
df_features$indicator_importance <- case_when(
df_features$indicator_order <= 3 ~ "High",
df_features$indicator_order <= 6 ~ "Medium",
df_features$indicator_order >= 7 ~ "Low",
TRUE ~ "Unknown"
)
cat("Indicator importance distribution:\n")
print(table(df_features$indicator_importance))
# 4. VALUE - Target variable analysis and transformation
cat("\n=== VALUE (TARGET) ANALYSIS ===\n")
value_summary <- summary(df_features$value)
print(value_summary)
# Check distribution and skewness
library(moments)
value_skewness <- skewness(df_features$value, na.rm = TRUE)
cat("Value skewness:", round(value_skewness, 3), "\n")
# Create value categories for classification tasks
value_quantiles <- quantile(df_features$value, c(0.33, 0.67), na.rm = TRUE)
df_features$value_category <- case_when(
df_features$value <= value_quantiles[1] ~ "Low",
df_features$value <= value_quantiles[2] ~ "Medium",
df_features$value > value_quantiles[2] ~ "High",
TRUE ~ "Unknown"
)
cat("Value category distribution:\n")
print(table(df_features$value_category))
# Apply log transform if highly skewed
if (abs(value_skewness) > 2) {
df_features$value_log <- log1p(abs(df_features$value))
cat("Applied log transform (log1p) due to high skewness\n")
} else {
df_features$value_log <- df_features$value
cat("No log transform needed\n")
}
# 5. ENGINEERED FEATURES
# Sample size tiers based on denominator_unweighted
df_features$sample_size_tier <- case_when(
df_features$denominator_unweighted < SAMPLE_SIZE_SMALL ~ "Small",
df_features$denominator_unweighted < SAMPLE_SIZE_LARGE ~ "Medium",
df_features$denominator_unweighted >= SAMPLE_SIZE_LARGE ~ "Large",
TRUE ~ "Unknown"
)
# Data quality score combining precision and is_preferred
df_features$data_quality_score <- (
(df_features$high_precision * 0.6) +  # 60% weight for high precision
(df_features$is_preferred * 0.4)      # 40% weight for preferred estimate
)
cat("\n=== ENGINEERED FEATURES SUMMARY ===\n")
cat("Sample size tier distribution:\n")
print(table(df_features$sample_size_tier))
cat("\nData quality score summary:\n")
print(summary(df_features$data_quality_score))
cat("\n=== NUMERIC FEATURE ENGINEERING COMPLETE ===\n")
cat("=== SCALING NUMERIC VARIABLES ===\n")
# Define numeric variables to scale
numeric_to_scale <- c(
"value_log",           # Target variable (use log-transformed if available)
"precision",
"characteristic_order",
"indicator_order",
"data_quality_score"
)
# Check which variables exist and have variance
existing_numeric <- numeric_to_scale[numeric_to_scale %in% names(df_features)]
cat("Variables to scale:", paste(existing_numeric, collapse = ", "), "\n")
# Show pre-scaling statistics
cat("\nPre-scaling statistics:\n")
pre_scaling_stats <- df_features[existing_numeric] %>%
summarise(across(everything(), list(
mean = ~ round(mean(.x, na.rm = TRUE), 3),
sd = ~ round(sd(.x, na.rm = TRUE), 3),
min = ~ round(min(.x, na.rm = TRUE), 3),
max = ~ round(max(.x, na.rm = TRUE), 3)
)))
print(pre_scaling_stats)
# Apply StandardScaler (mean=0, std=1)
scaled_variables <- df_features[existing_numeric] %>%
mutate(across(everything(), ~ scale(.x)[,1]))
# Add suffix "_scaled" to scaled variables
names(scaled_variables) <- paste0(names(scaled_variables), "_scaled")
# Add scaled variables to feature dataframe
df_features <- bind_cols(df_features, scaled_variables)
# Show post-scaling statistics
cat("\nPost-scaling statistics:\n")
post_scaling_stats <- scaled_variables %>%
summarise(across(everything(), list(
mean = ~ round(mean(.x, na.rm = TRUE), 3),
sd = ~ round(sd(.x, na.rm = TRUE), 3),
min = ~ round(min(.x, na.rm = TRUE), 3),
max = ~ round(max(.x, na.rm = TRUE), 3)
)))
print(post_scaling_stats)
cat("\n=== SCALING COMPLETE ===\n")
cat("Added", ncol(scaled_variables), "scaled variables with '_scaled' suffix\n")
cat("=== PREPARING FINAL FEATURE SET ===\n")
# Combine all dummy variables
all_dummies <- cbind(by_var_dummies, type_dummies, char_dummies)
cat("Combined dummy variables:", ncol(all_dummies), "features\n")
# Create comprehensive feature dataset
final_features <- bind_cols(
# Original identifier
df_features %>% select(data_id),
# Target variables
df_features %>% select(value, value_log, value_category),
# Scaled numeric features
scaled_variables,
# Binary features
df_features %>% select(is_preferred, high_precision),
# Label encoded categorical features
df_features %>% select(
indicator_encoded,
survey_cohort,
dataset_source_encoded
),
# Engineered categorical features
df_features %>% select(
precision_quality,
char_order_quintile,
indicator_importance,
sample_size_tier
),
# One-hot encoded features
as.data.frame(all_dummies)
)
# Create modeling-ready dataset (only features, no identifiers)
modeling_features <- final_features %>%
select(-data_id, -value, -value_category)  # Remove ID and keep only scaled value
cat("\nFinal feature set dimensions:", dim(final_features), "\n")
cat("Modeling feature set dimensions:", dim(modeling_features), "\n")
# Feature summary
cat("\n=== FEATURE SET SUMMARY ===\n")
cat("Total features:", ncol(final_features) - 1, "\n")  # Exclude data_id
cat("- Scaled numeric features:", ncol(scaled_variables), "\n")
cat("- Binary features:", 2, "\n")  # is_preferred, high_precision
cat("- Label encoded features:", 3, "\n")  # indicator, survey_cohort, dataset_source
cat("- Engineered categorical features:", 4, "\n")  # precision_quality, etc.
cat("- One-hot encoded features:", ncol(all_dummies), "\n")
# Check for missing values
missing_summary <- final_features %>%
summarise(across(everything(), ~ sum(is.na(.x)))) %>%
pivot_longer(everything(), names_to = "feature", values_to = "missing_count") %>%
filter(missing_count > 0)
if (nrow(missing_summary) > 0) {
cat("\nFeatures with missing values:\n")
print(missing_summary)
} else {
cat("\nNo missing values in final feature set\n")
}
# Export datasets
cat("\n=== EXPORTING DATASETS ===\n")
# Export comprehensive feature set
write_csv(final_features, file.path(outputs_path, "final_features_comprehensive.csv"))
cat("Exported: final_features_comprehensive.csv\n")
# Export modeling-ready features
write_csv(modeling_features, file.path(outputs_path, "modeling_features.csv"))
cat("Exported: modeling_features.csv\n")
# Export feature metadata
feature_metadata <- tibble(
feature_name = names(final_features),
feature_type = case_when(
grepl("_scaled$", feature_name) ~ "numeric_scaled",
feature_name %in% c("is_preferred", "high_precision") ~ "binary",
feature_name %in% c("indicator_encoded", "survey_cohort", "dataset_source_encoded") ~ "label_encoded",
feature_name %in% c("precision_quality", "char_order_quintile", "indicator_importance", "sample_size_tier") ~ "categorical_engineered",
grepl("^(by_var_|type_|char_)", feature_name) ~ "one_hot_encoded",
feature_name %in% c("value", "value_log") ~ "target",
feature_name == "value_category" ~ "target_categorical",
feature_name == "data_id" ~ "identifier",
TRUE ~ "other"
),
description = case_when(
feature_name == "data_id" ~ "Unique record identifier",
feature_name == "value" ~ "Original target variable (health indicator value)",
feature_name == "value_log" ~ "Log-transformed target variable",
feature_name == "value_category" ~ "Categorical target (Low/Medium/High)",
feature_name == "is_preferred" ~ "Binary: preferred estimate flag",
feature_name == "high_precision" ~ "Binary: high precision measurement (<=1 decimal)",
feature_name == "indicator_encoded" ~ "Label encoded: health indicator type",
feature_name == "survey_cohort" ~ "Label encoded: survey cohort identifier",
feature_name == "dataset_source_encoded" ~ "Label encoded: source dataset",
TRUE ~ "Feature engineered from original data"
)
)
write_csv(feature_metadata, file.path(outputs_path, "feature_metadata.csv"))
cat("Exported: feature_metadata.csv\n")
cat("\n=== FEATURE ENGINEERING COMPLETE ===\n")
cat("Ready for modeling with", nrow(final_features), "records and",
ncol(modeling_features), "features\n")
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
suppressPackageStartupMessages({
library(readr); library(dplyr); library(tidyr); library(moments)
})
# Auto-detect paths
current_dir <- basename(getwd())
if (current_dir == "Task_04") {
cleaned_data_path <- "../../Data/02_Cleaned"
outputs_path <- "../../Data/03_Scaled"
} else {
cleaned_data_path <- "02_Project/Data/02_Cleaned"
outputs_path <- "02_Project/Data/03_Scaled"
}
if (!dir.exists(outputs_path)) dir.create(outputs_path, recursive = TRUE)
# Parameters
RARE_CATEGORY_THRESHOLD <- 0.05
SAMPLE_SIZE_SMALL <- 1000
SAMPLE_SIZE_LARGE <- 10000
# Load the 7 selected cleaned datasets
selected_datasets <- c(
"access-to-health-care_national_zaf", "immunization_national_zaf", "hiv-behavior_national_zaf",
"water_national_zaf", "dhs-quickstats_national_zaf", "toilet-facilities_national_zaf", "child-mortality-rates_national_zaf"
)
cleaned_csv_files <- paste0(file.path(cleaned_data_path, selected_datasets), "_final.csv")
if (length(cleaned_csv_files[file.exists(cleaned_csv_files)]) == 0) {
cleaned_csv_files <- paste0(file.path(cleaned_data_path, selected_datasets), ".csv")
}
# Load and combine datasets
datasets <- lapply(cleaned_csv_files[file.exists(cleaned_csv_files)], function(file_path) {
df <- read_csv(file_path, show_col_types = FALSE)
df$dataset_source <- tools::file_path_sans_ext(basename(file_path))
return(df)
})
combined_df <- bind_rows(datasets)
cat("Combined", length(datasets), "datasets:", nrow(combined_df), "records,", ncol(combined_df), "fields\n")
df_features <- combined_df
# Categorical encoding
df_features$indicator_encoded <- as.numeric(as.factor(df_features$indicator))
df_features$survey_cohort <- as.numeric(as.factor(df_features$denominator_unweighted))
df_features$dataset_source_encoded <- as.numeric(as.factor(df_features$dataset_source))
# Group rare categories in by_variable_id
by_var_counts <- table(df_features$by_variable_id)
rare_threshold <- RARE_CATEGORY_THRESHOLD * nrow(df_features)
rare_categories <- names(by_var_counts[by_var_counts < rare_threshold])
df_features$by_variable_id_grouped <- ifelse(
df_features$by_variable_id %in% rare_categories, "Other", df_features$by_variable_id
)
# Create dummy variables
by_var_dummies <- model.matrix(~ by_variable_id_grouped - 1, data = df_features)
type_dummies <- model.matrix(~ indicator_type - 1, data = df_features)
char_dummies <- model.matrix(~ characteristic_category - 1, data = df_features)
colnames(by_var_dummies) <- paste0("by_var_", gsub("by_variable_id_grouped", "", colnames(by_var_dummies)))
colnames(type_dummies) <- paste0("type_", gsub("indicator_type", "", colnames(type_dummies)))
colnames(char_dummies) <- paste0("char_", gsub("characteristic_category", "", colnames(char_dummies)))
cat("Encoded categorical variables:", ncol(by_var_dummies) + ncol(type_dummies) + ncol(char_dummies), "dummy variables created\n")
# Create engineered numeric features
df_features$high_precision <- as.numeric(df_features$precision <= 1)
df_features$char_order_quintile <- if(max(df_features$characteristic_order, na.rm = TRUE) > 10) {
ntile(df_features$characteristic_order, 5)
} else {
df_features$characteristic_order
}
df_features$indicator_importance <- case_when(
df_features$indicator_order <= 3 ~ "High",
df_features$indicator_order <= 6 ~ "Medium",
TRUE ~ "Low"
)
# Target variable processing
value_skewness <- skewness(df_features$value, na.rm = TRUE)
df_features$value_log <- if(abs(value_skewness) > 2) log1p(abs(df_features$value)) else df_features$value
df_features$value_category <- cut(df_features$value, breaks = quantile(df_features$value, c(0, 0.33, 0.67, 1), na.rm = TRUE),
labels = c("Low", "Medium", "High"), include.lowest = TRUE)
# Additional engineered features
df_features$sample_size_tier <- case_when(
df_features$denominator_unweighted < SAMPLE_SIZE_SMALL ~ "Small",
df_features$denominator_unweighted < SAMPLE_SIZE_LARGE ~ "Medium",
TRUE ~ "Large"
)
df_features$data_quality_score <- (df_features$high_precision * 0.6) + (df_features$is_preferred * 0.4)
# Scale numeric variables
numeric_vars <- c("value_log", "precision", "characteristic_order", "indicator_order", "data_quality_score")
scaled_data <- df_features[numeric_vars] %>% mutate(across(everything(), ~ as.numeric(scale(.))))
names(scaled_data) <- paste0(names(scaled_data), "_scaled")
cat("Created", ncol(scaled_data), "scaled numeric features\n")
cat("Target variable skewness:", round(value_skewness, 3),
if(abs(value_skewness) > 2) " (log transformed)" else " (no transform)", "\n")
# Combine all features into final dataset
all_dummies <- cbind(by_var_dummies, type_dummies, char_dummies)
final_features <- bind_cols(
df_features %>% select(data_id, value, value_log, value_category),
scaled_data,
df_features %>% select(is_preferred, high_precision, indicator_encoded, survey_cohort, dataset_source_encoded),
df_features %>% select(char_order_quintile, indicator_importance, sample_size_tier),
as.data.frame(all_dummies)
)
# Create modeling-ready dataset (features only)
modeling_features <- final_features %>% select(-data_id, -value, -value_category)
# Export datasets
write_csv(final_features, file.path(outputs_path, "final_features_comprehensive.csv"))
write_csv(modeling_features, file.path(outputs_path, "modeling_features.csv"))
# Summary
cat("Final dataset:", nrow(final_features), "records,", ncol(modeling_features), "features\n")
cat("- Scaled numeric:", ncol(scaled_data), "\n")
cat("- Categorical encoded:", ncol(all_dummies) + 6, "\n")
cat("- Exported: modeling_features.csv (ready for ML)\n")
cat("Final training file: modeling_features.csv\n")
cat("Records:", nrow(modeling_features), "| Features:", ncol(modeling_features), "\n\n")
cat("Sample data (first 5 rows, first 8 columns):\n")
kable(head(modeling_features[1:8], 5), digits = 3)
