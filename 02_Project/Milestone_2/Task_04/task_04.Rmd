---
title: "Task 4: Correlation Analysis and Feature Importance"
author: "BIN381 - Data Analysis Dashboard Project"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_float: true
    code_folding: show
    theme: flatly
    fig_width: 10
    fig_height: 8
  pdf_document:
    toc: true
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE,
                      fig.width = 10, fig.height = 8)
```

# Introduction

This report conducts correlation analysis between variables and determines feature importance to weight attributes based on their relevance to modeling requirements. This analysis focuses on the core Task 4 deliverables as defined in the project requirements.

## Task 4 Deliverables

1. **Correlation Analysis Results** - Identify relationships between health indicators
2. **Feature Importance Rankings** - Rank features by discriminatory power
3. **R/R Markdown Code** - Statistical analysis implementation

## Methodology

- **Correlation Analysis**: Pearson correlation coefficients (threshold: |r| > 0.7)
- **Feature Importance**: Variance analysis to measure discriminatory power
- **Weight Attribution**: Evidence-based recommendations for modeling phase

# Data Loading and Setup

```{r libraries}
# Load required libraries
suppressMessages({
  library(tidyverse)
  library(corrplot)
  library(pheatmap)
  library(DT)
  library(kableExtra)
})

cat("Analysis environment ready\n")
```

```{r load-data}
# Set data path
data_path <- "../../Data/01_Raw/"

# List all CSV files
csv_files <- list.files(data_path, pattern = "*.csv", full.names = TRUE)
cat("Found", length(csv_files), "CSV files\n")

# Load all datasets
datasets <- list()
for (file in csv_files) {
  dataset_name <- gsub(".csv", "", basename(file))
  datasets[[dataset_name]] <- read_csv(file, show_col_types = FALSE)
}

cat("Loaded", length(datasets), "datasets successfully\n")
```

# Data Consolidation and Preparation

```{r consolidate-data}
# Combine all datasets
combined_data <- bind_rows(datasets, .id = "dataset_source")

# Clean and prepare data for analysis
analysis_data <- combined_data %>%
  filter(!is.na(Value)) %>%
  select(dataset_source, Indicator, Value, SurveyYear) %>%
  mutate(
    Value = as.numeric(Value),
    SurveyYear = as.numeric(SurveyYear)
  ) %>%
  filter(!is.na(Value), !is.infinite(Value))

cat("Combined dataset:", nrow(analysis_data), "observations\n")

# Check indicator coverage
indicator_coverage <- analysis_data %>%
  group_by(Indicator) %>%
  summarise(
    n_datasets = n_distinct(dataset_source),
    n_observations = n(),
    .groups = 'drop'
  ) %>%
  arrange(desc(n_datasets))

cat("Total unique indicators:", nrow(indicator_coverage), "\n")

# Save indicator coverage for review
write.csv(indicator_coverage, "Data/indicator_coverage_summary.csv", row.names = FALSE)
cat("Indicator coverage saved to Data/indicator_coverage_summary.csv\n")
```

```{r prepare-correlation-matrix}
# Use dataset_source as grouping for better correlation analysis
correlation_matrix_data <- analysis_data %>%
  group_by(Indicator, dataset_source) %>%
  summarise(avg_value = mean(Value, na.rm = TRUE), .groups = 'drop') %>%
  pivot_wider(names_from = Indicator, values_from = avg_value) %>%
  select(-dataset_source)

# Handle missing values and constant columns
correlation_matrix_data <- correlation_matrix_data %>%
  mutate_all(~ ifelse(is.na(.), mean(., na.rm = TRUE), .)) %>%
  select_if(~ var(., na.rm = TRUE) > 0)

cat("Correlation matrix dimensions:", nrow(correlation_matrix_data), "x", ncol(correlation_matrix_data), "\n")

# Preview the correlation matrix data structure
cat("Sample of indicators included in correlation analysis:\n")
print(names(correlation_matrix_data)[1:min(5, ncol(correlation_matrix_data))])
```

# Correlation Analysis Results

```{r correlation-analysis}
# Calculate correlation matrix
numeric_data <- correlation_matrix_data %>% select_if(is.numeric)
cor_matrix <- cor(numeric_data, use = "complete.obs")

cat("Final correlation matrix:", nrow(cor_matrix), "x", ncol(cor_matrix), "\n")
cat("Range of correlations:", round(min(cor_matrix, na.rm = TRUE), 3),
    "to", round(max(cor_matrix, na.rm = TRUE), 3), "\n")

# Save correlation matrix for review
write.csv(cor_matrix, "Data/correlation_matrix.csv")
cat("Correlation matrix saved to Data/correlation_matrix.csv\n")
```

## Correlation Heatmap

```{r correlation-heatmap, fig.cap="Correlation heatmap showing relationships between health indicators"}
# Create correlation heatmap
if (ncol(cor_matrix) > 1) {
  pheatmap(cor_matrix,
           main = "Correlation Heatmap of Health Indicators",
           color = colorRampPalette(c("red", "white", "blue"))(100),
           breaks = seq(-1, 1, length.out = 101),
           display_numbers = TRUE,
           fontsize = 10,
           angle_col = 45,
           number_format = "%.2f")

  cat("Heatmap generated successfully\n")
} else {
  cat("Insufficient data for correlation heatmap\n")
}
```

**Heatmap Interpretation:**
- Red colors indicate negative correlations
- Blue colors indicate positive correlations
- White indicates no correlation (near zero)
- Numbers show exact correlation coefficients
- Perfect correlations (1.0) appear on the diagonal

## Significant Correlations Analysis

```{r significant-correlations}
# Find significant correlations
cor_threshold <- 0.7
significant_cors <- which(abs(cor_matrix) > cor_threshold & cor_matrix != 1, arr.ind = TRUE)

if (nrow(significant_cors) > 0) {
  significant_pairs <- data.frame(
    Indicator1 = rownames(cor_matrix)[significant_cors[,1]],
    Indicator2 = colnames(cor_matrix)[significant_cors[,2]],
    Correlation = cor_matrix[significant_cors],
    Strength = ifelse(cor_matrix[significant_cors] > 0, "Positive", "Negative"),
    Abs_Correlation = abs(cor_matrix[significant_cors])
  ) %>%
    arrange(desc(Abs_Correlation))

  cat("SIGNIFICANT CORRELATIONS FOUND:\n")
  cat("Total significant correlations (|r| >", cor_threshold, "):", nrow(significant_pairs), "\n")
  cat("Strongest positive correlation:", round(max(significant_pairs$Correlation), 3), "\n")
  if (any(significant_pairs$Correlation < 0)) {
    cat("Strongest negative correlation:", round(min(significant_pairs$Correlation), 3), "\n")
  }

  # Display as formatted table
  kable(significant_pairs,
        caption = paste("Significant Correlations (|r| >", cor_threshold, ")"),
        digits = 3) %>%
    kable_styling(bootstrap_options = c("striped", "hover"))

  # Save significant correlations
  write.csv(significant_pairs, "Data/significant_correlations.csv", row.names = FALSE)
  cat("Significant correlations saved to Data/significant_correlations.csv\n")

} else {
  cat("NO SIGNIFICANT CORRELATIONS FOUND\n")
  cat("No correlations above threshold of", cor_threshold, "\n")
  cat("Features are relatively independent (beneficial for modeling)\n")
}
```

# Feature Importance Analysis

```{r feature-importance}
# Calculate feature variance as importance measure
feature_variance <- numeric_data %>%
  summarise_all(~ var(., na.rm = TRUE)) %>%
  pivot_longer(everything(), names_to = "Feature", values_to = "Variance") %>%
  arrange(desc(Variance)) %>%
  mutate(
    Rank = row_number(),
    Normalized_Variance = scales::rescale(Variance, to = c(0, 1)),
    Combined_Score = Normalized_Variance,
    Final_Rank = Rank
  )

cat("Total features analyzed:", nrow(feature_variance), "\n")
cat("Ranking method: Variance-based importance\n")

# Display top features
kable(feature_variance,
      caption = "Feature Importance Rankings by Variance",
      digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Save feature importance results
write.csv(feature_variance, "Data/feature_importance_rankings.csv", row.names = FALSE)
cat("Feature importance rankings saved to Data/feature_importance_rankings.csv\n")
```

## Feature Importance Visualization

```{r feature-plot, fig.cap="Feature importance ranked by variance (discriminatory power)"}
# Create feature importance plot
importance_plot <- ggplot(feature_variance,
                         aes(x = reorder(Feature, Combined_Score), y = Combined_Score)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  coord_flip() +
  labs(
    title = "Health Indicator Importance Rankings",
    subtitle = "Ranked by variance (higher variance = greater discriminatory power)",
    x = "Health Indicators",
    y = "Normalized Importance Score"
  ) +
  theme_minimal() +
  theme(
    axis.text.y = element_text(size = 9),
    plot.title = element_text(size = 14, face = "bold"),
    plot.subtitle = element_text(size = 11, color = "gray50")
  )

print(importance_plot)

cat("Feature importance plot generated\n")
```

**Plot Interpretation:**
- Features are ranked from highest to lowest discriminatory power
- Normalized scores range from 0 to 1
- Higher bars indicate features with greater variance (more informative for modeling)
- Top-ranked features should receive higher weights in modeling phase

# Weight Attribution Strategy

```{r weight-strategy}
# Create weighting strategy based on importance rankings
weight_strategy <- feature_variance %>%
  mutate(
    Weight_Category = case_when(
      Final_Rank <= 5 ~ "High Weight (3x)",
      Final_Rank <= 15 ~ "Medium Weight (2x)",
      Final_Rank <= 30 ~ "Standard Weight (1x)",
      TRUE ~ "Low Weight (0.5x)"
    ),
    Suggested_Weight = case_when(
      Final_Rank <= 5 ~ 3.0,
      Final_Rank <= 15 ~ 2.0,
      Final_Rank <= 30 ~ 1.0,
      TRUE ~ 0.5
    )
  ) %>%
  select(Feature, Final_Rank, Combined_Score, Weight_Category, Suggested_Weight)

# Weight distribution summary
weight_summary <- weight_strategy %>%
  count(Weight_Category, name = "Number_of_Features") %>%
  arrange(desc(Number_of_Features))

cat("WEIGHT DISTRIBUTION SUMMARY:\n")
kable(weight_summary,
      caption = "Distribution of Features Across Weight Categories") %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Detailed weight recommendations
cat("\nDETAILED WEIGHT RECOMMENDATIONS:\n")
kable(weight_strategy,
      caption = "Suggested Feature Weights for Modeling Phase",
      digits = 4) %>%
  kable_styling(bootstrap_options = c("striped", "hover"))

# Save weight strategy
write.csv(weight_strategy, "Data/weight_attribution_strategy.csv", row.names = FALSE)
cat("Weight strategy saved to Data/weight_attribution_strategy.csv\n")
```

**Weight Strategy Rationale:**
- **High Weight (3x)**: Top 5 features with highest discriminatory power
- **Medium Weight (2x)**: Features ranked 6-15, still significant contribution
- **Standard Weight (1x)**: Features ranked 16-30, baseline contribution
- **Low Weight (0.5x)**: Lower-ranked features, reduced influence

This evidence-based approach ensures that features with greater variance (and thus more information content) have proportionally greater influence in the modeling phase.

# Task 4 Summary and Results

```{r task-summary}
cat("=== TASK 4 DELIVERABLES COMPLETED ===\n\n")

cat("CORRELATION ANALYSIS RESULTS:\n")
cat("- Correlation matrix dimensions:", nrow(cor_matrix), "x", ncol(cor_matrix), "\n")
cat("- Correlation range:", round(min(cor_matrix, na.rm = TRUE), 3),
    "to", round(max(cor_matrix, na.rm = TRUE), 3), "\n")

if (exists("significant_pairs") && nrow(significant_pairs) > 0) {
  cat("- Significant correlations found:", nrow(significant_pairs), "\n")
  cat("- Multicollinearity considerations required for modeling\n")
} else {
  cat("- No significant correlations above threshold\n")
  cat("- Features are relatively independent (ideal for modeling)\n")
}

cat("\nFEATURE IMPORTANCE RANKINGS:\n")
cat("- Total features analyzed:", nrow(feature_variance), "\n")
cat("- Top feature:", feature_variance$Feature[1], "\n")
cat("- Ranking method: Variance-based discriminatory power\n")

cat("\nWEIGHT ATTRIBUTION STRATEGY:\n")
for (i in 1:nrow(weight_summary)) {
  cat("- ", weight_summary$Weight_Category[i], ":", weight_summary$Number_of_Features[i], "features\n")
}

cat("\nR/R MARKDOWN CODE:\n")
cat("- Statistical analysis implementation complete\n")
cat("- All code documented and reproducible\n")
cat("- Results exported to CSV files for integration\n")

cat("\n=== INTEGRATION WITH TASK 5 ===\n")
cat("Key findings ready for data selection criteria refinement:\n")
cat("1. Correlation insights for multicollinearity management\n")
cat("2. Evidence-based feature importance hierarchy\n")
cat("3. Quantitative weight recommendations for modeling\n")
```

```{r save-results}
# Save key R objects for Task 5 integration
save(feature_variance, cor_matrix, weight_strategy, significant_pairs,
     file = "Data/task_04_results.RData")

cat("All results saved to Data/task_04_results.RData for Task 5 integration\n")
cat("CSV files created in Data/ folder for external review and documentation\n")
```

# Conclusions

This correlation analysis and feature importance evaluation provides a statistical foundation for data preparation decisions in the modeling phase. The analysis successfully delivers all three required Task 4 deliverables:

1. **Correlation Analysis Results**: Complete correlation matrix with identification of significant relationships
2. **Feature Importance Rankings**: Evidence-based ranking of health indicators by discriminatory power
3. **R/R Markdown Code**: Fully documented and reproducible statistical analysis

## Key Findings

- **Data Coverage**: Successfully analyzed `r nrow(analysis_data)` observations across `r length(datasets)` datasets
- **Feature Independence**: `r if(exists("significant_pairs") && nrow(significant_pairs) > 0) paste("Found", nrow(significant_pairs), "significant correlations requiring multicollinearity consideration") else "Features show good independence, beneficial for modeling"`
- **Importance Hierarchy**: Clear ranking established with top features showing substantially higher discriminatory power
- **Evidence-Based Weighting**: Quantitative approach provides objective foundation for feature weighting in modeling phase

## Next Steps

These findings will be integrated into Task 5's data selection criteria refinement process, ensuring that correlation insights and importance rankings inform the final dataset optimization for modeling requirements.

---

*Analysis completed on `r Sys.Date()` for BIN381 Data Analysis Dashboard Project - Milestone 2, Task 4*